{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9349a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward at t=0: 2.355\n",
      "Mean reward at t=24: 2.17\n",
      "Mean p at t=0: 0.46796011426346934\n",
      "Mean p at t=24: 0.3896721130871995\n",
      "Actions (t=0..2, agents 0..4):\n",
      "[[0 1 1 1 1]\n",
      " [0 1 0 1 1]\n",
      " [0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import numpy as np\n",
    "from PDGenv import PDGEnv\n",
    "from agents import make_agents\n",
    "from loop import run_episode\n",
    "\n",
    "# 1) Create environment\n",
    "env = PDGEnv(size=10)  # 100 agents\n",
    "\n",
    "# 2) Create agents (choose parameters)\n",
    "A = 0.5\n",
    "beta = 1.0\n",
    "epsilon = 0.2\n",
    "agents = make_agents(env.n_agents, A=A, beta=beta, epsilon=epsilon, p_init=0.5)\n",
    "\n",
    "# 3) Run 1 episode\n",
    "actions_hist, rewards_hist, p_hist = run_episode(env, agents, tmax=25, seed=123)\n",
    "\n",
    "# 4) Print sanity info\n",
    "print(\"Mean reward at t=0:\", rewards_hist[0].mean())\n",
    "print(\"Mean reward at t=24:\", rewards_hist[-1].mean())\n",
    "print(\"Mean p at t=0:\", p_hist[0].mean())\n",
    "print(\"Mean p at t=24:\", p_hist[-1].mean())\n",
    "\n",
    "# Show first 10 actions of first 5 agents for first 3 timesteps\n",
    "print(\"Actions (t=0..2, agents 0..4):\")\n",
    "print(actions_hist[:3, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd69be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SANITY CHECK START =====\n",
      "\n",
      "[1] Check p bounds\n",
      "p min: 1.8749981389863832e-47\n",
      "p max: 1.0\n",
      "\n",
      "[2] Check reward bounds\n",
      "reward min: 0.0\n",
      "reward max: 5.0\n",
      "\n",
      "[3] All-Cooperate (all C) test\n",
      "Mean reward (expected 3): 3.0\n",
      "Unique rewards: [3.]\n",
      "\n",
      "[4] All-Defect (all D) test\n",
      "Mean reward (expected 1): 1.0\n",
      "Unique rewards: [1.]\n",
      "\n",
      "[5] One defector test\n",
      "Defector reward (expected 5): 5.0\n",
      "Neighbors of defector: [90, 10, 9, 1]\n",
      "Neighbors rewards (expected ~2.25): [2.25 2.25 2.25 2.25]\n",
      "Neighbors mean reward: 2.25\n",
      "\n",
      "===== SANITY CHECK END =====\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"===== SANITY CHECK START =====\")\n",
    "\n",
    "# 1) p bounds\n",
    "print(\"\\n[1] Check p bounds\")\n",
    "print(\"p min:\", p_hist.min())\n",
    "print(\"p max:\", p_hist.max())\n",
    "\n",
    "# 2) reward bounds\n",
    "print(\"\\n[2] Check reward bounds\")\n",
    "print(\"reward min:\", rewards_hist.min())\n",
    "print(\"reward max:\", rewards_hist.max())\n",
    "\n",
    "# 3) All-Cooperate test\n",
    "print(\"\\n[3] All-Cooperate (all C) test\")\n",
    "actions_all_C = np.ones(env.n_agents, dtype=int)\n",
    "rewards_all_C = env.step(actions_all_C)\n",
    "print(\"Mean reward (expected 3):\", rewards_all_C.mean())\n",
    "print(\"Unique rewards:\", np.unique(rewards_all_C))\n",
    "\n",
    "# 4) All-Defect test\n",
    "print(\"\\n[4] All-Defect (all D) test\")\n",
    "actions_all_D = np.zeros(env.n_agents, dtype=int)\n",
    "rewards_all_D = env.step(actions_all_D)\n",
    "print(\"Mean reward (expected 1):\", rewards_all_D.mean())\n",
    "print(\"Unique rewards:\", np.unique(rewards_all_D))\n",
    "\n",
    "# 5) One defector among cooperators\n",
    "print(\"\\n[5] One defector test\")\n",
    "actions_one_D = np.ones(env.n_agents, dtype=int)\n",
    "defector = 0\n",
    "actions_one_D[defector] = 0\n",
    "\n",
    "rewards_one_D = env.step(actions_one_D)\n",
    "print(\"Defector reward (expected 5):\", rewards_one_D[defector])\n",
    "\n",
    "neighbors = env.neighbors[defector]\n",
    "print(\"Neighbors of defector:\", neighbors)\n",
    "print(\"Neighbors rewards (expected ~2.25):\", rewards_one_D[neighbors])\n",
    "print(\"Neighbors mean reward:\", rewards_one_D[neighbors].mean())\n",
    "\n",
    "print(\"\\n===== SANITY CHECK END =====\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv) LearningDynamics_Paper",
   "language": "python",
   "name": "learningdynamics-paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
